---
alias: information
---

Information, as Andy Clark puts it in _Surfing Uncertainty_, is any energy transfer: light waves, sound waves, the apprehension of force by the nervous system. 

[[Norbert Wiener 1]]: "To live effectively is to live with adequate information. Thus, communication and control belong to the essence of man's inner life, even as they belong to his life in society".

[[Intelligence]] has evolved (or is designed, in the case of computers) to be altered by forms of energy (light waves, sound waves, etc) which would not normally affect it, and to be altered in a complex, contextual, memoried way. When intelligent agents create information for other intelligent agents, we call it [[communication]].

# Dedeo 2018: Information Theory for Intelligent People

> _H(X)_ is the basic quantity in information theory... [It] goes by a number of different names: "uncertainty," "information," even "entropy." [...It] is a fundamentally epistemic quantity. It quantifies how an actual agent in the world goes about gathering information about what is going on. 

The state of the world is always determinate, i.e. "certain"; rather, it is the _agent_ who is uncertain about its state.

> information is often called a "syntactic" theory, concerned only with the properties of symbols in abstraction from their meanings

> For a more formal definition of coarse graining, let’s say we have a set, _X_, with three options, _{a, b, c}_. I can talk about the uncertainty of a process that spits out one of these three symbols. But what if I don’t care about (or can’t tell) the difference between _b_ and _c_? Instead of making a finer distinction between _b_ and _c_, I just lump them together into some super-symbol _S_. If I coarse-grain _X_ in this way, I can talk about the uncertainty of probability distribution over the reduced set, _X0_ , _{a, S}_, where _S_ refers to “_b_ or _c_, I don’t care”, and _p~bc~_ is just _p~b~_ + _p~c~_.

Higher-information events both 1) take more description, and 2) reduce uncertainty more than lower-information events. When all possible states are of equal probability, there is a state of maximum information or entropy.