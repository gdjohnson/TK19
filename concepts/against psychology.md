# Yarkoni 2019: The generalizability crisis

## The genralizability crisis is a crisis of the Extended Way of Being

_See [[Extend-and-Retreat]]._

There are two modes of induction, Yarkoni writes. The first is the "fast" approach, which is "liberal and incautious" and "makes the default assumption that every observation can be safely generalized to other similar-seeming situations until such time as [...] contradicted by new evidence." The second is the slow approach, which is "conservative"; in this school of induction, "an observed relaitonship is assumed to hold only in situations identical, or very similar to, the one in which it has already been observed." In other words, one eagerly claims territory until contested, at which point it backs off; the other is modest and lays claim only to the amount of territory it can strongly defend. (See also [[Extend-and-Retreat]].)

> I think it's readily apparent that the vast majority of psychological scientists have long operated under a regime of (extremely) fast generalization. One has only to scan the table of contents of our journals to observe paper titles such as "Local Competition Amplifies the Corrosive Effects of Inequality" (Krupp & Cook 2018), "The Intent to Persuade Transforms Language via Emotionality" (Rocklage, Rucker, & Nordgren 2018), "and "Inspiration Encourages Belief in God" (Critcher & Lee 2018). [all titles from May 2018 _Psychological Science_ issue]

In other words, they are extending themselves too far, have been wooed and tempted into making false claims:

> It is not simply a matter of researchers slightly overselling their conclusions here or there; the fundamental problem is that, on close examination, a huge proportion of the verbal claims made in empirical psychology articles turn out to have very little relationship with the statistical quantities they putatively draw their support from.

Rather, researchers are systematically overselling their conclusions by a significant margin.

## [[surrogation]]

The first stage of [[surrogation]] is from the verbal construct under interest to its quantitative [[surrogation|operationalization]]. A rich, complex psychological construct, typically but not necessarily with its origins in folk psychology, is simplified to a metric which purports to represent it.

> verbally expressed psychological constructs—things like cognitive dissonance, language acquisition, and working memory capacity—cannot be directly measured with an acceptable level of objectivity and preccision. What _can_ be measured objectively and precisely are operationalizations of those constructs—for example, a performance score on a particular digit span task, or the number of English words an infant has learned by age 3. Trading vague verbal assertions for concrete measures and manipulations is what enables resaerchers to draw precise, objective quantitative inferences; however, the same move also introduces new points of potential failure, because the validity of the original verbal assertion now depends not only on what happens to be true about the world itself, but also on the degree to which the chosen [[surrogation|proxy]] measures successfully capture the constructs of interest—what psychometricians term construct validity.

Once the study is completed, the second stage occurs: the quantitative or operationalized reality is re-translated back into language via generalization or loose induction. The coarse metrics to some extent "disappear," as we re-enter the realm of language where knowledge is hosted and decisions made.

As in mangement [[surrogation]] (Choi, Hecher, Tayler), [[surrogation|Goodhart's Law]], or [[surrogation]] through gamification (Nguyen), the disruption or distraction caused by the [[surrogation]] relies on "how closely"—or distantly—"the verbal and quantitative expressions... align." It "is the mismatch between our generalization intention and the model specification"—in other words, between holism and quantitative representative—"that introduces an inflated risk of inferrential error, and not the model specification alone." (Yarkoni 2019).

## Variation, sampling, and generalizability 

Yarkoni argues that, though social psychologists see the randomization of study subjects as a necessary step in order to generalize across a population, there is "nothing special about subjects" that justifies this preferential treatment. 

> The tendency to ignore stimulus sampling variability has been discussed in the literature for over 50 years... and was influentially dubbed the _fixed-effect fallacy_ by (Clark, 1973) Unfortunately, outside of a few domains such as psycholinguistics, it remains rare to see psychologists model stimuli as random effects—despite the fact that most inferences researchers draw are clearly meant to generalize over entire populations of stimuli.

> Common design factors that researchers hardly ever vary, yet almost invariably intend to generalize over, include experimental task, between-subject instructional manipulation, research site, experimenter... [and ambient experimental environment] and so on and so forth effectively ad infinitum.

> Any time one samples design elements into one's study from a broader population of possible candidates, one introduces sampling error that is likely to influence the outcome of the study to some unknown degree.

To illustrate this, we will take Yarkoni's chosen example. A psychologist wishes to study, say, an effect like _verbal overshadowing_, the phenomenon whereby explicit verbal description of a visual memory actually decreases later recall accuracy. This psychologist picks a single scenario deemed representative, and shows a semi-randomized population sample a single video, with a single human face, and tests recall between individuals asked to verbally describe their recollection first, and those given a control task. When a statistically meaningful correlation between verbalization and recall ability is demonstrated, the psychologist publishes the study under the claim that evidence of verbal overshadowing has been found.

Yarkoni argues that in order to understand verbal overshadowing as a broad _phenomenon_, one must not only sample from a population of individuals, but from the population of scenarios in which we might expect verbal overshadowing to play out. (Otherwise we give only what has been called an "existence proof"—a demonstration that there is at least one possible situation in which an effect occurs, but which is far from proving any kind of prevalence or systematic effect in daily life.)

> The clear implication of such findings is that many literatures within psychology are likely to be populated by studies that have spuriously misattributed statistically significant effects to fixed effects of interest when they should actually be attributed to stochastic variation in uninteresting stimulus properties. Moreover, given that different sets of stimuli are liable to produce

In other words, we could imagine that certain faces, or certain encounters or presentations with that face, are more suceptible to verbal overshadowing, or that the specific control task chosen (e.g. naming country capitals) helps activate areas in memory that least those assigned to it to outperform those tasked with verbal description. We could imagine that verbal overshadowing might hold true of faces but not other objects. 

> For example, perhaps participants in Alogna et al's experimental condition felt greature pressure to produce the correct answer (having previously spent several minutes describing their perceptions), and it was the stress rather than the treatment _per se_ that resulted in poorer performance. Or, perhaps the effect had nothing at all to do with the treatment condition and instead reflected a poor choice of control condition (say, because naming countries and capitals incidentally activates helpful memory consolidation processes). And so on and so forth. (A skeptic might object that such explanations are not as plausible as the verbal overshadowing account, but this misses the point: [...]  reality is not under any obligation to only manifest sparse causal relationships that researchers find intuitive!)

What's more, the fat tail of these many minority probabilities adds up, such that almost no such study has the statistical grounds to generalize up from the specific situation to the general phenomenon under study (e.g. verbal overshadowing). It is the same as if we had studied many types of verbal overshadowing in a single subject: generalization to the population would be irresponsible, just as generalization to the full phenomenon is irresponsible from the specific behavior of a specific instance.

> The strict conclusion we are entitled to draw, given the limitations of the experimental design inherited from [the verbal overshadowing study by] Scooler and Engstler-Schooler (1990), is that there is at least one particular video containing one particular face that, when followed by one particular lineup of faces, is more difficult for participants to identify if they previously verbally described the appearance of the target face than if they were asked to name countries and capitals. 

> A recognition memory task that uses just one video, one target face, and one set of foils simply cannot provide a meaningful test of a broad construct like verbal overshadowing.

What is worse, Yarkoni believes we should expect _a priori_ that there will be at least some studies in which recollection improves, and at least some in which it worsens, thus the "existence proof" gives us very little new information:

> the latter conclusion is statistically bound to be true given even very conservative background assumptions about the [[surrogation|operationalization]], and also that one can argue from first principles—i.e., _without any data at all_—that there must be _many_ stimuli that show a so-called verbal overshadowing effect.

(From the lessons of [[conceptual analysis]] and conceptual engineering, in philosophy, we can import another important and related insight: organic human concepts, including those which make up folk psychology discourse, do not contain essences; there is no defining them by a few simple criteria which can cover all reasonable uses of the term. For terms become polysemous; over a long lifehistory, they are inflated for rhetorical flourish, metaphorically applied to cover an [[excess of signification]], or straight-up misused. Thus we can imagine that, in order to make valid claims to describing the phenomenon that, in the folk sphere, is called anger, authoritarianism, or domestic abuse, a researcher must randomly sample uses in the folk language to find representative "coverage" of instances of the general principle. Alternatively, upon finding multiple distinct meanings or senses under one "handle," as in identifiers like "sexual assault," a researcher may need to [[linguistic conquests]], identifying the specific sub-phenomenon and refusing, out of integrity, to lay claim to describing the conflationary whole.)

### Rebuttal to unwarranted generalizability

An important dimension of the problem is that psychology as a field (1) is at least tacitly aware of many of these limitations, and (2) in the aggregate, does tend toward the kind of justified generalizability that Yarkoni yearns for. A study on verbal overshadowing testing one specific set of experimental designs will, if it shows promise, frequently be followed up on either by the original, or another new, research team. New situations will be designed, and the many design variables will over time by varied such that, eventually, there will be some legitimate case for the larger phenomenon holding. Moreover, psychologists acknowledge, or pay lip service, to the danger of extrapolating from a small handful of studies, and to the hazards of generalization.

The response to this rebuttal is two-fold: One is that mere acknowledgment of _some_ obfuscating effect of design variability is not equivalent to _full acknowledgment of the scope of the problem_. If one is to take the _scope_ of the statistical problem Yarkoni presents seriously, then it casts almost all touted psychology findings back in the WIP bin, and shifts the pace of discoveries in the field from one in which a talented researcher could plausibly make several discoveries in a career, to one in which it will inevitably take several generations of research careers in order to make a single discovery. It is the difference, as Michael Inzlicht and Yoel Inbar describe it, between believing your resarch is building bricks (for some eventual building...) and believing that it is building grains of sand, which might one day form a brick structurally sound enough to build with. Every evidenced behavior in psychologists contradicts such a mindset of pace and limitation. Instead, psychologists advocate for their ability to meaningfully advise in the public sphere; they deliver TED talks which claim to unveil the secrets of emotionality or social life; their studies shape public _and_ corporate policy (see the recent institutionalization of IAT, or the destructive policy advise of behavioral economists in the COVID-19 pandemic). Their paper titles write checks their statistics cannot possibly cash. 

Paul Meehl's words, in his own attacks on social psychology, as quoted by Yarkoni reveal the extent of this scope clash, preventing easy reconciliations of the "already aware" kind:

> I think that for most faculty in soft psychology the full acceptance of my line of thought would involve a painful realization that one has achieved some notoriety, tenure, economic security and the like by engaging, to speak bluntly, in a bunch of nothing.

What is needed is a sense of pace layers. In geologic time, the inorganic features of the planet watch as their organic colleagues flourish and bloom, wither and die, thousands of cycles time-lapsing around them, as their own surfaces are eroded by wind and water, as cracks appear in the rockface. This is how psychology, done responsibly, ought to understand its research practice.

# Statistics as a performance

I argue that Yarkoni's paper also shows, somewhat obliquely, the role of appearance gamification (_cf._ Nguyen) in psychology.

The _validity_ of an _[[surrogation|operationalization]]_ of a _psychological construct_ is its ability to meaningfully measure that construct—for instance, the ability of an researcher-designed authoritarianism seven-point scale to meaningfully measure the presence of the construct that is "authoritarianism." Most research in social psychology, Yarkoni writes, is satisfied with "face validity"—it concludes that "if a particular [[surrogation|operationalization]] _seems_ like it has something to do with the construct of interest, then it is an acceptable stand-in for that construct.

There is also the implication, later in the paper, and advanced in other critiques as to the "performative" or "show business" nature of social psychology, that much of the empirical work is a performance of legitimacy, more than a scientifically rigorous inquiry into the subject matter. Yarkoni, noting the hopeful side of this, believes that much of psychological research has been essentially the development of researchers' qualitative theories _featuring the rhetorical support of_ the empirical studies that accompany (and hence, legitimize) them. In other words, the field has _surrogated_ a holistic but hard-to-measure quality like "insight" and cartographic accuracy for one's ability to perform the necessary empiricism: good research can and is still done, but it is crowded by talented performers with very little of insight to give the field. In Yarkoni's words:

> I contend that a good deal of what currently passes for empirical psychology is already best understood as insightful qualitative analysis dressed up as shoddy quantitative science. 

> the inferential statistics so often reported in soft psychology articles primarily serve as a performative ritual intended to convince one's colleagues and/or one's self that something very scientific and important is taking place.

## Conclusion

> Under such circumstances, it's unclear why anyone should really care about the inferential statistics psychologists report in most papers, seeing as those statistics bear only the most tenuous of connections to authors' sweeping verbal conclusions.

The discipline has "resolved to run roughshod over the relationship between our verbal theories and their corresponding quantitative specifications." 

One way to summarize the problem in psychology is that its researchers have overextended their findings (_see [[Extend-and-Retreat]] entry_), have written checks they cannot catch. Indeed, the first piece of advice Yarkoni gives researchers is to “draw more conservative inferences”—to “replace the hasty generalizations” with “more cautious conclusions.” “Papers should be given titles like ‘Transient manipulation of self-reported anger influences small hypothetical charitable donations,’ and not ones like ‘Hot head, warm heart: Anger increases economic charity.’” But of course, there is little [[status|prestige]] or power to be had in claiming so little knowledge: all the incentives of turf point toward bold ambitious and barefaced audacity over intellectual “modesty.”

> The overarching conclusion is that many fields of psychology currently operate under a kind of collective self-deception, using a thin sheen of quantitative rigor to mask inferences that remain, at their core, almost entirely qualitative.

# Inbar, Inzlicht, Sommers, & Pizarro: Contextualism and psychology

## The value of falsificationist frameworks vs. desciptive noticing accounts

> Is it productive for us [psychologists] to jam our empirical research into this falsificationist framework that arguably is only well-suited to more mature disciplines that can make more specific predictions, and where the auxiliary assumptions are really well understood? We're at a point now where we don't understand enough of what's going on for that framework to even be useful.

> Getting back to the JDM stuff, which is a literature I love and think is quite strong, you take this stuff into the field and half the time it doesn't work. Gain versus loss framing: there's a ton of research showing that people attend more to losses than to equivalent gains, and yet you do the field study where you frame something as a gain or a loss and it's like, you know, sometimes it works sometimes it doesn't; if it doesn't, God knows why not. If you're at that stage, maybe it's best to stake a step back from a super formal hypothetical deductive framework and say, Let's see what's out there and try to describe it accurately; in a specific situation what effects might we be able to generate.

## Prediction as the ultimate goal

> The point always is to accumulate knowledge, and to be able to understand and predict, and if it doesn't add up to that why do it? ...I think it's maybe more productive to do predictive work, and then do focused studies on exactly the thing we care about.

## Overapplying weak findings in public policy

> [[typification|stereotype]] threat researchers wrote an amicus brief in _Fisher v Texas_ saying, It's been show that stereotype threat has these wide-ranging and pernicious effects on the performance of minorities. And that may be true or not, but what they're basing that on is these very specific lab stories that have been designed to elicit stereotype threat effects, and in fact when you do bigger field experiments, you often get really mixed or null results.

On many psych studies as "existence proofs" that a phenomenon which is _already believed_ to pertain in some cases, and not pertain in others, is demonstrated to hold in at least one case. An existence proof is a bit like a distortion of those grade school, rudimentary statistics problems: "If one has a bag with four green balls, and two yellow balls, and reaches inside to find a yellow ball, what is the probability that the next drawn ball is also yellow?" Except here, one has no idea the composition of the bag, and—reaching in to find a yellow ball—declares that the bag is filled with yellow balls.

> If what you're claiming is [a study] proves an existence of something [that] wasn't obvious to policymakers, that's a contribution I guess, but [a journalist] could do an existence proof, or any nonfiction writers; I think the problem is the reason [governments] are receptive to [psychology findings] is it has this layer of supposed objectivity which it probably doesn't deserve, at least with something as complicated and messy as how to handle the COVID pandemic.

> Is it an established study that generalizes to real-life situations, or is this just something you could say, like a Freudian could say, "no, it's very hard to influence behavior because a lot of these things are deeply welded to their unconscious, and if they gave them the authority they give you, that could equally get them to influence their policies in the proper way; the problem is that you get to do this [tell the government how to write policy] only because you get this extra bit of credibility that doesn't seem is warranted based on where [psychology as a] field is.

## On the natural incentives toward extend-and-retreat

> I don't wanna shoot myself in the foot; we [psychologists] do plenty of judgment and decision-making work, and I think there are robust effects, but we're not trying to predict real-world behavior under a confluence of causes... And to the extent that we pretend that's what we're doing, we're doing it to big-up ourselves as relevant to society.

_If quality control is not based on results, then what is it based on? [[status|Prestige]] and respect among one's peers, through the all-present [[plausibility]] heuristic._

## On people not wanting to look

> For advertising, for policy, it's crazy how little people pay attention the outcome. With advertising for instance, the stakeholders, the people coming up with the ads, don't give a fuck whether the ad works or not. They go on to the next job, and if it doesn't work no one wants to admit it... But there is an answer to whether that ad increased sales or not.

> The weakest claim I can make in support of social science is we should be collecting data all the time, and if we're implementing policy we should be measure the effect. 

The pair of podcasters discuss what it means to "jump off the cliff" and follow through on their ideas as psychologists in the field:

> A certain episode you said, "what we're [psychology] is a bit of a show," and you were talking about experiments. And it was like, woah, listen to what you just said, we're doing a show? This isn't real, but we're pretending it's real? That's a damning thing to say, but nothing that came after followed from that... That the thing most psychologists do is a show. [...] I don't know what it would look like to act on that belief in your field. 

# Inzlicht 2016: Getting Better

> As someone who has been doing research for nearly twenty years, I now can’t help but wonder if the topics I chose to study are in fact real and robust. Have I been chasing puffs of smoke for all these years?

On the three main sources of psychology's replication crisis:

> Our problems are not small and they will not be remedied by small fixes. Our problems are systemic and they are at the core of how we conduct our science. My eyes were first opened to this possibility when I read Simmons, Nelson, and Simonsohn’s paper during what seems like a different, more innocent time. This paper details how small, seemingly innocuous, and previously encouraged data-analysis decisions could allow for anything to be presented as statistically significant. That is, flexibility in data collection and analysis could make even impossible effects seem possible and significant.
>
> What is worse, Andrew Gelman made clear that a researcher need not actively p-hack their data to reach erroneous conclusions. It turns out such biases in data analyses might not be conscious, that researchers might not even be aware of how their data-contingent decisions are warping the conclusions they reach. This is flat-out scary: Even honest researchers with the highest of integrity might be reaching erroneous conclusions at an alarming rate.
> 
> Third, is the problem of publication bias. As a field, we tend only to publish significant results. This could be because as authors we choose to focus on these; or, more likely, because reviewers, editors, and journals force us to focus on these and to ignore nulls. This creates the infamous file drawer that altogether warps the research landscape. Because it is unclear how large the file drawer is for any research literature, it is hard to determine how large or small any effect is, if it exists at all.
>
> I think these three ideas—that data flexibility can lead to a raft of false positives, that this process might occur without researchers themselves being aware, and the unknown size of the file drawer—explains why so many of our cherished results can’t replicate. These three ideas suggest we might have been fooling ourselves into thinking we were chasing things that are real and robust, when we were pursuing neither.

On **stereotype threat**:

> I edited an entire book on [[typification#stereotype threat|stereotype threat]], I have signed my name to an amicus brief to the Supreme Court of the United States citing stereotype threat, yet now I am not as certain as I once was about the robustness of the effect. I feel like a traitor for having just written that; like, I’ve disrespected my parents, a no no according to Commandment number 5. But, a meta-analysis published just last year suggests that stereotype threat, at least for some populations and under some conditions, might not be so robust after all. 

# Very Bad Wizards in conversation with Four Beers, Two Psychologists

> the other day Micky you said psychology was all a bit of a show. A show!! And you’re right! But what the fuck does that say about what we’re doing here.

> it’s true, the experiments are just performance to the thinking. Sometimes I wonder if I’d be better off doing close description

# Alvaro de Menard 2020: What's Wrong with Social Science

de Menard worked in the Replication Markets for a year, skimming shy of 3000 studies from across the social sciences. He reports that, with high accuracy and despite lacking domain expertise, he was able to identify replicating vs non-replicating papers in just about 2.5 minutes.

Because he believes the markers of non-replicating papers are so obvious, he dismisses the usual "[[garden of forking paths]]" or "researcher degrees of freedom" complaints, arguing that researchers who regularly put out non-replicating work are conscious of doing so, and ought to be considered frauds accordingly:

> People within the academy don't want to rock the boat. They still have to attend the conferences, secure the grants, publish in the journals, show up at the faculty meetings: all these things depend on their peers. When criticising bad research it's easier for everyone to blame the forking paths rather than the person walking them. No need for uncomfortable unpleasantries. The fraudster can admit, without much of a hit to their reputation, that indeed they were misled by that dastardly [[garden]], really through no fault of their own whatsoever, at which point their colleagues on twitter will applaud and say "ah, good on you, you handled this tough situation with such exquisite virtue, this is how progress happens! hip, hip, hurrah!" What a ridiculous charade.

![[citationsVsReplications.png]]

> Whatever the explanation might be, the fact is that the academic system does not allocate citations to true claims.

> You might hypothesize that the citations of non-replicating papers are negative, but negative citations are extremely rare. One study puts the rate at 2.4% (Catalini, Lacetera, Oettl 2015). Astonishingly, even after retraction the vast majority of citations are positive, and those positive citations continue for decades after retraction.

What does correlate strongly with citation rates is positive findings—those that confirm the hypothesis; see Duyx, Urlings et al 2017.

![Replications vs journal prestige](replicationsVsJournalPrestige.png)

While only about half of papers replicate, a large portion of replicating papers fall victim to model misspecification problems, causality issues and overgeneralization, external validity (i.e. outside the lab, or else are true in a trivially obvious way.

> Nunnally ([[1960]]) noted various problems with null hypothesis testing, underpowered studies, over-reliance on student samples (it doesn't take Joe Henrich to notice that using Western undergrads for every experiment might be a bad idea), and much more. The problem (or excuse) of publish-or-perish, which some portray as a recent development, was already in place by this time.: _The "reprint race" in our universities induces us to publish hastily-done, small studies and to be content with inexact estimates of relationships._

> Paul Meehl (1967) is highly insightful on problems with null hypothesis testing in the social sciences, the "crud factor", lack of theory, etc. Meehl (1970) brilliantly skewers the erroneous (and still common) tactic of automatically controling for "confounders" in observational designs without understanding the causal relations between the variables. Meehl (1990) is downright brutal: he highlights a series issues which, he argues, make psychological theories "uninterpretable". He covers low standards, pressure to publish, low power, low prior probabilities, and so on: _I am prepared to argue that a tremendous amount of taxpayer money goes down the drain in research that pseudotests theories in soft psychology and that it would be a material social advance as well as a reduction in what Lakatos has called “intellectual pollution” if we would quit engaging in this feckless enterprise._

# Reading "Ignorance, a Skilled Practice"
![["Reading 'Ignorance, A Skilled Practice'"]]

## Rozin, Scott et al 2014: Asymmetrical Social Mach Bands

> For almost 100 years, psychology as a discipline has been jockeying to be classified as a natural science. The natural sciences are often viewed as more prestigious and more advanced than the social sciences, and psychologists have strived to be viewed as “baby” natural scientists rather than the most “scientific” social scientists. In recent decades, psychology has appended the word science to itself (psychological science) and to former subareas that have now become departments (e.g., cognitive science, neuroscience). This trend in nomenclature is also illustrated in the names of several relatively new journals in psychology: Social Psychological and Personality Science, Perspectives on Psychological Science, Current Directions in Psychological Science, and Clinical Psychological Science. And unlike textbooks in more secure natural sciences, introductory psychology textbooks usually include a section on what science is and what the scientific method is, implicitly or explicitly claiming that psychology is a (natural) science.1 We think psychologists are defending their belief that psychology is properly categorized as a natural science; in contrast, physicists, biologists, and chemists, who are more firmly placed in the natural sciences, rarely feel compelled to defend their disciplines as natural sciences. This phenomenon is an instance of a general social phenomenon: the tendency for border or marginal members of positively valenced groups to emphasize their membership in those groups.