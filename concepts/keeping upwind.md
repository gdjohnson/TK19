---
aliases: [intrinsic empowerment]
---

_See also [[Agency]] entry._

"Keeping upwind" is Paul Graham's term for choosing the best set or range of future options. See also [[intelligence as futures optimization|mutual modeling of futures]]; Wrench's argument about social & linguistic non-commitment. All these concepts rely on a vision of [[intelligence as futures optimization]].

> Instead of working back from a goal, work forward from promising situations. This is what most successful people actually do anyway.

> In the graduation-speech approach, you decide where you want to be in twenty years, and then ask: what should I do now to get there? I propose instead that you don't commit to anything in the future, but just look at the options available now, and choose those that will give you the most promising range of options afterward.

> Flying a glider is a good metaphor here. Because a glider doesn't have an engine, you can't fly into the wind without losing a lot of altitude. If you let yourself get far downwind of good places to land, your options narrow uncomfortably. As a rule you want to stay upwind. So I propose that as a replacement for "don't give up on your dreams." Stay upwind.

# Chris Marais: Empowerment as Intrinsic Motivation

Empowerment refers to one's amount of [[agency]], the amount of possible, positive futures one can bring about. _See also [[The Garden of Forking Paths]]._

> Having money, influential friends, or owning a vehicle means that you are more empowered to decide what kind of future you want to live. It doesn’t necessarily mean that you know which goals are the right ones, but it certainly puts you in a place of power, where many possible futures are available to choose from. This concept of empowerment was formalised in the context of designing adaptive agents by Klyubin et al. [1]. It is intended as a kind of goal-independent intrinsic motivator for behaviour, and has produced some interesting results for robotics, reinforcement learning and adaptive systems generally. 

And:

> all else being equal, according to Klyubin, agents should maximise the number of possible future outcomes of their actions. “Keeping your options open” in this way means that when a task does arise, one is as empowered as possible to carry out whatever needs to be done to complete it. Klyubin et al. present the concept nicely in two aptly titled papers: “All Else Being Equal Be Empowered”[1] and “Keep Your Options Open: An Information-Based Driving Principle for Sensorimotor Systems”[3]. Since then, a lot of exciting work in robotics and reinforcement learning have used and extended the concept [8,9].

## A crisis in values leads to generic empowerment

This, more or less, is the state our society is in: values are unclear, i.e. no one knows exactly which "direction" in the [[garden]] to invest in. People desire, then, not specific things but rather to keep their options open, to hang out in a position in the maze where, when or if they eventually decide to, they can invest and strike out in a specific direction. In some cases we call this behavior "perpetual adolescence." Make more money, climb the corporate ladder, date more people—why? No one's quite sure. But best to keep your options open... who knows what will happen in the future, it could open doors...

# Strategic noncommitment

_See also the concept of self-binding, or strategic commitment, in [[Strategic Interaction]]._

Strategic noncommitment is the idea that, in social interactions, we expect to be bound to our words, and therefore limit the scope of commitments (either to future actions, or to current states) to keep options open—to stay empowered—if required to self-justify. This theory casts [[justification]] as a centerpiece of social life, where our leash is our ability to justify (particularly to authority figures such as bosses, teachers, police, etc—but also in general, socially).

# Khetarpal 2020: A Theory of Affordances in Reinforcement Learning

Following Gibson (1977) Khetarpal et al theorize 

> we need to formalize Gibson’s intuition of “action success”. Because we would like affordances to generalize across environments with different rewards, we start by considering a notion of “intent” of an action, and we consider an action to have succeeded if it realizes its [[intent]]. For example, having a coffee machine affords the action of making coffee, because we can successfully obtain coffee from the machine. This does not necessarily mean the action is desirable in the current context: if the agent must go to sleep soon, or has an upset stomach, the reward for drinking coffee might be negative. Nonetheless, the action itself can be executed and would result in the intended consequence of possessing coffee. This example gives the intuition that intent is best captured by thinking about a target state distribution that should be achieved after executing an action.

> In order to define affordances, we need to first capture the notion of what it would mean for an agent to carry out an action “successfully”. To do this, we introduce the notion of [[intent]], i.e., a desired outcome for an action. Affordances will then capture a subset of the state-action space in which the intent is achieved. This view is very compatible with model-based RL, in which the transition model captures the consequences of actions.

# Reserves

To-do: investigate the general concept of "reserves" in the sense of leaving military troops at the ready, behind main lines, to be slotted in at the last possible minute to make a decision, so that their placement is most effective.